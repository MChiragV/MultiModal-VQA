{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11801135,"sourceType":"datasetVersion","datasetId":7407267}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers peft bitsandbytes datasets evaluate\n!pip install qwen-vl-utils[decord]==0.0.8\n!pip install evaluate\n!pip install rouge_score\n!pip install bert-score\n!git clone https://github.com/neulab/BARTScore.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/BARTScore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"#  Import libraries\nimport os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import (\n    AutoProcessor,\n    AutoTokenizer,\n    Qwen2_5_VLForConditionalGeneration,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import LoraConfig, get_peft_model\nfrom qwen_vl_utils import process_vision_info\nfrom peft import PeftModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PATHS","metadata":{}},{"cell_type":"code","source":"# Paths and model setup\nCSV_PATH     = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"\nIMAGE_FOLDER = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"\nMODEL_ID     = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the model","metadata":{}},{"cell_type":"code","source":"# Load processor & tokenizer\nprocessor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_ID, \n    trust_remote_code=True,\n    use_fast=False\n)\nprint(\"â†’ image placeholder is:\", tokenizer.additional_special_tokens[0])\n\n# Load base model\nbase_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    MODEL_ID,\n    load_in_8bit=True,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules='all-linear',\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"class QwenDataset(Dataset):\n    def __init__(self, csv_path, image_folder):\n        self.df = pd.read_csv(csv_path)\n        self.image_folder = image_folder\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_folder, row[\"image_name\"])\n        image = Image.open(image_path).convert(\"RGB\")\n        instruction = (\n            \"You must answer with exactly one word. \"\n            \"Spaces between words will be treated as multiple words. \"\n            \"Do not include any explanations or punctuation.\"\n        )\n        question = row[\"question\"]\n        full_prompt = f\"{instruction}\\n{question}\"\n        answer = row[\"answer\"]\n\n        text = f\"<|im_start|>user\\n<image>\\n{full_prompt}<|im_end|>\\n<|im_start|>assistant\\n{answer}<|im_end|>\"\n\n        return {\n            \"image\": image,\n            \"text\": text,\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset loading\nds = QwenDataset(CSV_PATH, IMAGE_FOLDER)\ntotal_len = len(ds)\n\n# Compute sizes\ntrain_size = int(0.8 * total_len)\nval_size = int(0.1 * total_len)\ntest_size = total_len - train_size - val_size  # Remaining for test\n\n# Set random seed\nseed = 42\ngenerator = torch.Generator().manual_seed(seed)\n\n# Perform split with fixed random seed\ntrain_ds, val_ds, test_ds = random_split(ds, [train_size, val_size, test_size], generator=generator)\n\n# Print sizes\nprint(train_size)\nprint(val_size)\nprint(test_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"#Paths\n\nLORA_WEIGHTS = \"sohith18/qwen2vl-lora-vqa-7b\"  # Updated to match your output dir\nMODEL_ID     = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n\n#Load base model & LoRA weights\nbase_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    MODEL_ID,\n    load_in_8bit=True,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nmodel = base_model\nmodel.eval()  # Set to evaluation mode\n\n#Load processor and tokenizer\nprocessor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, use_fast=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_question_and_answer(text):\n    user_match = re.search(r\"<\\|im_start\\|>user\\n(?:<image>\\n)?(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n    raw_user = user_match.group(1).strip() if user_match else \"\"\n\n    # Get only the last question sentence (assumes question is last line)\n    question_lines = [line.strip() for line in raw_user.splitlines() if line.strip()]\n    question = question_lines[-1] if question_lines else \"\"\n\n    assistant_match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n    answer = assistant_match.group(1).strip() if assistant_match else \"\"\n\n    return question, answer\n\ndef predict(image_path, question,image=None):\n    if image is None:\n        image = Image.open(image_path).convert(\"RGB\")\n\n    messages = [{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": image},\n            {\"type\": \"text\", \"text\": question}\n        ]\n    }]\n\n    # Process the messages correctly\n    processed_text = processor.apply_chat_template([messages[0]], tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    \n    # Create inputs\n    inputs = processor(\n        text=processed_text,\n        images=image_inputs,\n        videos=video_inputs,\n        return_tensors=\"pt\",\n    ).to(model.device)\n\n    # Get the length of input for extracting only new tokens\n    input_len = inputs[\"input_ids\"].shape[1]\n     # Generate with proper parameters\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=20,  # Increased from 20\n            do_sample=False,\n            num_beams=1,         # Increased from 1\n            eos_token_id=tokenizer.eos_token_id,  # Explicitly set EOS token\n        )\n\n    # Extract only the new tokens (the answer)\n    generated_tokens = output[0][input_len:]\n\n    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    \n    \n    return answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing with one Sample","metadata":{}},{"cell_type":"code","source":"# Load eval data\nCSV_PATH = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"\nIMAGE_FOLDER = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"\ndf = pd.read_csv(CSV_PATH)\ndf[\"image_path\"] = df[\"image_name\"].apply(lambda x: os.path.join(IMAGE_FOLDER, x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 4\nq,ea = extract_question_and_answer(test_ds[i][\"text\"])\npred_answer = predict(None,q,test_ds[i][\"image\"])\nprint(f'Question: {q}')\nfirst_word = re.split(r'\\W+', pred_answer.strip())[0]\nprint(f'Predicted Answer: {first_word}')\nprint(f'Expected Answer: {ea}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\nrow = df.iloc[45]\nans = predict(row[\"image_path\"], row[\"question\"])\n\n# Extract only the first word (split on non-word characters)\nfirst_word = re.split(r'\\W+', ans.strip())[0]\n\nprint(\"Q:\", row[\"question\"])\nprint(\"A:\", first_word)\nprint(\"GT:\", row[\"answer\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom bart_score import BARTScorer\nfrom sentence_transformers import SentenceTransformer, util\nimport evaluate\n\n# Load metrics\nrouge = evaluate.load(\"rouge\")\nbertscore = evaluate.load(\"bertscore\")\nembedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\n\n# Metric functions\ndef compute_exact_match(pred, label):\n    return int(pred.strip().lower() == label.strip().lower())\n\ndef compute_token_f1(pred, label):\n    pred_tokens = pred.strip().lower().split()\n    label_tokens = label.strip().lower().split()\n    common = set(pred_tokens) & set(label_tokens)\n    if len(common) == 0:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(label_tokens)\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n\ndef compute_semantic_similarity(pred, label):\n    emb_pred = embedding_model.encode(pred, convert_to_tensor=True)\n    emb_label = embedding_model.encode(label, convert_to_tensor=True)\n    return float(util.cos_sim(emb_pred, emb_label))\n\n\ndef clean_answer(predicted_answer):\n    # Lowercase and extract alphabetic substrings only\n    tokens = re.findall(r'[a-zA-Z]+', predicted_answer)\n\n    if not tokens:\n        return \"Unknown\"\n\n    # Remove duplicates while keeping order\n    seen = set()\n    tokens = [t for t in tokens if not (t in seen or seen.add(t))]\n\n\n    return tokens[0]\n\n\ndef extract_question_and_answer(text):\n    user_match = re.search(r\"<\\|im_start\\|>user\\n(?:<image>\\n)?(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n    raw_user = user_match.group(1).strip() if user_match else \"\"\n\n    # Get only the last question sentence (assumes question is last line)\n    question_lines = [line.strip() for line in raw_user.splitlines() if line.strip()]\n    question = question_lines[-1] if question_lines else \"\"\n\n    assistant_match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n    answer = assistant_match.group(1).strip() if assistant_match else \"\"\n\n    return question, answer\n\n\n# Main evaluation\ndef evaluate_qwen(eval_dataset):\n    exact_matches = []\n    token_f1s = []\n    rouge_scores = []\n    bert_scores = []\n    bart_scores = []\n    semantic_similarities = []\n\n    for sample in tqdm(eval_dataset, desc=\"Evaluating\"):\n        image = sample[\"image\"]\n        text = sample[\"text\"]\n        question, expected_answer = extract_question_and_answer(text)\n\n        # print(question, expected_answer)\n\n        # Get prediction\n        pred_raw = predict(None,question,image)\n        pred_answer = clean_answer(pred_raw)\n\n        print(f'Question: {question}')\n        print(f'Predicted Answer: {pred_answer}')\n        print(f'Expected Answer: {expected_answer}')\n\n        # Metrics\n        exact_matches.append(compute_exact_match(pred_answer, expected_answer))\n        token_f1s.append(compute_token_f1(pred_answer, expected_answer))\n\n        rouge_result = rouge.compute(predictions=[pred_answer], references=[expected_answer], use_stemmer=True)\n        rouge_scores.append(rouge_result[\"rougeL\"])\n\n        bert_result = bertscore.compute(predictions=[pred_answer], references=[expected_answer], lang=\"en\")\n        bert_scores.append(bert_result[\"f1\"][0])\n\n        bart_score = bart_scorer.score([pred_answer], [expected_answer])[0]\n        bart_scores.append(bart_score)\n\n        semantic_similarities.append(compute_semantic_similarity(pred_answer, expected_answer))\n\n    # Summary\n    print(\"\\n Evaluation Metrics:\")\n    print(f\"  - Exact Match:            {np.mean(exact_matches):.4f}\")\n    print(f\"  - Token-level F1:         {np.mean(token_f1s):.4f}\")\n    print(f\"  - ROUGE-L:                {np.mean(rouge_scores):.4f}\")\n    print(f\"  - BERTScore (F1):         {np.mean(bert_scores):.4f}\")\n    print(f\"  - BARTScore:              {np.mean(bart_scores):.4f}\")\n    print(f\"  - Semantic Cosine Sim.:   {np.mean(semantic_similarities):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Subset\n\nsubset_500 = Subset(test_ds, range(500))\nevaluate_qwen(subset_500)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}