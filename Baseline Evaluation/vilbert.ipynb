{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11653434,"sourceType":"datasetVersion","datasetId":7313251}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VilBERT","metadata":{}},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install pandas pillow torch transformers nltk evaluate sentencepiece tqdm sentence-transformers rouge_score bert_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"import os\ninput_dir = \"/kaggle/input/dataset/\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T06:55:15.772152Z","iopub.execute_input":"2025-05-18T06:55:15.772412Z","iopub.status.idle":"2025-05-18T06:55:15.776035Z","shell.execute_reply.started":"2025-05-18T06:55:15.772390Z","shell.execute_reply":"2025-05-18T06:55:15.775217Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Sample testing for pretrained VilBERT model","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\nimport torch\n\n# Load model and processor once\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n# Example dataset\nimage_paths = [\n    \"https://storage.googleapis.com/allennlp-public-data/vqav2/baseball.jpg\",\n    \"https://storage.googleapis.com/allennlp-public-data/vqav2/vqa-examples/kitchen.jpg\",\n]\nquestions = [\n    \"What game are they playing?\",\n    \"What is the color of the flower?\",\n]\n\n# Results list\nresults = []\n\n# Loop through dataset\nfor img_path, question in zip(image_paths, questions):\n    # Load image (from URL or local path)\n    if img_path.startswith(\"http\"):\n        image = Image.open(requests.get(img_path, stream=True).raw)\n    else:\n        image = Image.open(img_path)\n\n    # Encode and predict\n    encoding = processor(image, question, return_tensors=\"pt\")\n    outputs = model(**encoding)\n    pred = model.config.id2label[outputs.logits.argmax(-1).item()]\n\n    # Save result\n    results.append({\n        \"image\": img_path,\n        \"question\": question,\n        \"answer\": pred\n    })\n\n# Display results\nfor r in results:\n    print(f\"Q: {r['question']} \\nA: {r['answer']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T06:55:18.024110Z","iopub.execute_input":"2025-05-18T06:55:18.024663Z","iopub.status.idle":"2025-05-18T06:55:47.807648Z","shell.execute_reply.started":"2025-05-18T06:55:18.024639Z","shell.execute_reply":"2025-05-18T06:55:47.803698Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 06:55:30.121076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747551330.303274      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747551330.359076      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0e6427a7fa462f921c5ad9d1f8c5bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fabdaf7d0e0c45188a97c89efbe8859f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19be8684530e416d838c8c796fab4fba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9dd8665dfcb4bafb639d3044c72daee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19fc5bfda4824d8cbb877c2090908de9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72031c32b9164c70bf1df10806e775ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b2b3c38597431792a8894957ed4171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/470M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4132bd061a66442695bb7ce3c1955fc2"}},"metadata":{}},{"name":"stdout","text":"Q: What game are they playing? \nA: baseball\n\nQ: What is the color of the flower? \nA: orange\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Run the VilBERT pretrained model","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nfrom PIL import Image\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu\n\ninput_dir = \"/content\"\n\nDATA_CSV = input_dir + '/cleaned_data.csv'\nIMAGE_DIR = input_dir + '/images/images/'\n# Load your CSV\ndf = pd.read_csv(DATA_CSV)\ndf.columns = [\"image_path\", \"question\", \"expected_answer\"]\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Load model and move to GPU\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(device)\n\nresults = []\npredictions = []\nexpected_answers = []\n\nfor idx, row in df.iterrows():\n    img_path = IMAGE_DIR + row[\"image_path\"]\n    question = row[\"question\"]\n    expected = row[\"expected_answer\"]\n\n    image = Image.open(img_path).convert(\"RGB\")\n    encoding = processor(image, question, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():  # Disable gradient tracking for inference\n        outputs = model(**encoding)\n\n    pred = model.config.id2label[outputs.logits.argmax(-1).item()]\n\n    results.append({\n        \"image_path\": img_path,\n        \"question\": question,\n        \"expected_answer\": expected,\n        \"predicted_answer\": pred\n    })\n\n    # Append answers for BLEU score calculation\n    predictions.append(pred.lower().split())  # Split to word tokens for BLEU\n    expected_answers.append([expected.lower().split()])  # Wrap expected in a list\n\n\n# Convert to DataFrame\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"vqa_predictions.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"# evaluate_vqa.py\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\n\nimport evaluate\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# ——— 0) Read your CSV ———\ndf = pd.read_csv(\"/content/vqa_predictions.csv\")\npredictions = df[\"predicted_answer\"].fillna(\"\").astype(str).tolist()\nreferences  = df[\"expected_answer\"].fillna(\"\").astype(str).tolist()\n\n# ——— 1) Load metrics & models ———\nrouge           = evaluate.load(\"rouge\")\nbertscore       = evaluate.load(\"bertscore\")\nembedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Prepare BART for “BARTScore”\ndevice     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer  = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n\ndef compute_bart_score(candidates, sources, batch_size=8, num_beams=4):\n    \"\"\"\n    Rough emulation of BARTScore:\n     - Treat `sources` as the encoder input\n     - Treat `candidates` as the decoder target\n     - Compute the average log-probability per token\n    \"\"\"\n    scores = []\n    for i in range(0, len(candidates), batch_size):\n        src_batch  = sources[i : i+batch_size]\n        cand_batch = candidates[i : i+batch_size]\n\n        # tokenize encoder inputs (references)\n        enc = tokenizer(src_batch,  return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        # tokenize decoder targets (predictions) as labels\n        labs = tokenizer(cand_batch, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n        # ignore pad tokens in loss\n        labs[labs == tokenizer.pad_token_id] = -100\n\n        with torch.no_grad():\n            out = bart_model(**enc, labels=labs)\n        # out.loss is the average -logprob per non-ignored token\n        avg_neg_logprob = out.loss.detach().cpu().item()\n        # count valid tokens\n        valid_tokens = (labs != -100).sum().item()\n        # total logprob across the batch\n        total_logprob = -avg_neg_logprob * valid_tokens\n        # average logprob per token\n        avg_logprob = total_logprob / valid_tokens if valid_tokens > 0 else 0.0\n\n        # assign that same score to each example in the batch\n        scores.extend([avg_logprob] * len(src_batch))\n\n    return scores\n\n# ——— 2) Helper metric functions ———\ndef compute_exact_match(pred, label):\n    return int(pred.strip().lower() == label.strip().lower())\n\ndef compute_token_f1(pred, label):\n    p_tok = pred.strip().lower().split()\n    l_tok = label.strip().lower().split()\n    common = set(p_tok) & set(l_tok)\n    if not common:\n        return 0.0\n    prec = len(common) / len(p_tok)\n    rec  = len(common) / len(l_tok)\n    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n\ndef compute_semantic_similarity(pred, label):\n    e1 = embedding_model.encode(pred,   convert_to_tensor=True)\n    e2 = embedding_model.encode(label, convert_to_tensor=True)\n    return float(util.cos_sim(e1, e2))\n\ndef clean_answer(ans):\n    ans = re.sub(r\"\\b\\w+:\\s*\", \"\", ans)\n    ans = re.sub(r\"^[^a-zA-Z]+\", \"\", ans)\n    return re.split(r\"[?\\n]\", ans)[0].strip()\n\n# ——— 3) Storage ———\nexact_matches         = []\ntoken_f1s             = []\nrouge_scores          = []\nbert_scores           = []\nsemantic_similarities = []\n\n# ——— 4) Loop over your CSV data ———\nfor pred_raw, expected in tqdm(zip(predictions, references),\n                               total=len(predictions),\n                               desc=\"Evaluating\"):\n    # 1) clean\n    pred = clean_answer(pred_raw)\n\n    # 2) fallback: if cleaning emptied it, use the raw string\n    if pred == \"\":\n        pred = pred_raw.strip()\n\n    # Exact Match & Token‑F1\n    exact_matches.append(compute_exact_match(pred, expected))\n    token_f1s.    append(compute_token_f1(pred, expected))\n\n    # ROUGE‑L fmeasure\n    r = rouge.compute(\n        predictions=[pred],\n        references=[expected],\n        use_stemmer=True\n    )[\"rougeL\"]\n    rouge_scores.append(r)\n\n    # BERTScore (F1)\n    b = bertscore.compute(\n        predictions=[pred],\n        references=[expected],\n        lang=\"en\"\n    )[\"f1\"][0]\n    bert_scores.append(b)\n\n    # Semantic Cosine\n    semantic_similarities.append(compute_semantic_similarity(pred, expected))\n\n# ——— 5) Compute BARTScore over all pairs ———\nbart_scores = compute_bart_score(predictions, references, batch_size=8)\nbart_avg    = np.mean(bart_scores)\n\n# ——— 6) Print summary ———\nprint(\"\\n🔍 Evaluation Metrics:\")\nprint(f\"  - Exact Match:            {np.mean(exact_matches):.4f}\")\nprint(f\"  - Token-level F1:         {np.mean(token_f1s):.4f}\")\nprint(f\"  - ROUGE-L:                {np.mean(rouge_scores):.4f}\")\nprint(f\"  - BERTScore (F1):         {np.mean(bert_scores):.4f}\")\nprint(f\"  - BARTScore (avg lm‐logp): {bart_avg:.4f}\")\nprint(f\"  - Semantic Cosine Sim.:   {np.mean(semantic_similarities):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}