{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11801135,"sourceType":"datasetVersion","datasetId":7407267}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ac260700-6a21-42b0-9941-b9a08f6b7ed7","cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"id":"87d33c91-07d9-45a0-a41a-9ad488fad407","cell_type":"code","source":"!pip install -U transformers accelerate peft\n!pip install -U bitsandbytes\n!pip install -U evaluate\n!pip install -U rouge_score\n!pip install bert-score\n!git clone https://github.com/neulab/BARTScore.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"53c22132-adea-4478-9233-0827ea67d46f","cell_type":"markdown","source":"# Imports","metadata":{}},{"id":"a4be4456","cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nimport torch\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nfrom peft import LoraConfig, get_peft_model,PeftModel\nfrom transformers import TrainingArguments, Trainer\nimport os\nfrom torch.utils.data import Dataset,DataLoader \nfrom evaluate import load\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"19c722c5","cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nimage_folder = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"  \ncsv_path = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fff10770-8d98-4611-9f2b-85448234c6f9","cell_type":"markdown","source":"## Loading the models","metadata":{}},{"id":"9df9d618","cell_type":"code","source":"processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2ae38aab-4b97-462e-8152-b82eb63e8d7e","cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"id":"067092ae-dbb2-4ba8-b675-f319ffb9985e","cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, csv_path, image_folder, processor, max_samples=None):\n        self.data = pd.read_csv(csv_path)\n\n        if max_samples is not None:\n            self.data = self.data[:max_samples]  # Take only the first max_samples rows\n\n        self.image_folder = image_folder\n        self.processor = processor\n\n        print(f\"[INFO] Loaded {len(self.data)} samples from '{csv_path}'\")\n        print(f\"[INFO] Image folder: {image_folder}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        image_path = os.path.join(self.image_folder, row['image_name'])\n        image = Image.open(image_path).convert(\"RGB\")\n    \n        question = row['question']\n        answer = row['answer']\n    \n        # Prepare inputs (question + image)\n        inputs = self.processor(images=image, text=question, return_tensors=\"pt\", \n                                padding=\"max_length\", truncation=True, max_length=128)\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n    \n        # Tokenize answer with padding and truncation\n        tokenized = self.processor.tokenizer(\n            answer,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=10\n        )\n    \n        input_ids = tokenized[\"input_ids\"].squeeze(0)\n        labels = input_ids\n        inputs[\"labels\"] = labels\n\n        # Add metadata for later reference\n        inputs[\"image_name\"] = row[\"image_name\"]\n        inputs[\"question\"] = question\n        inputs[\"answer\"] = answer\n    \n        return inputs\n\n\n\n# Load dataset\nfull_dataset = VQADataset(csv_path, image_folder, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"644dbd13-f68d-47b0-a0e1-c4adc0a55161","cell_type":"code","source":"train_ratio = 0.8\nval_ratio = 0.075\ntest_ratio = 0.125\n\n# Total size of the dataset\ntotal_size = len(full_dataset)\n\n# Compute sizes\ntrain_size = int(train_ratio * total_size)\nval_size = int(val_ratio * total_size)\ntest_size = total_size - train_size - val_size  # Ensures total sums to len(full_dataset)\n\n# Perform the split\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    full_dataset, [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)  # for reproducibility\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0d307a5d-f18f-4c9c-b205-f3b425d8d623","cell_type":"markdown","source":"# Inference","metadata":{}},{"id":"593bfe2d-ce91-4ba1-8fd4-e3f04fa91e30","cell_type":"code","source":"from transformers import BlipForQuestionAnswering, BlipProcessor\nfrom peft import PeftModel\nimport torch\n\nlora_path = \"sohith18/blip-lora-vqa\"\n\n#Load BLIP base model in 8-bit with fp16 and auto device mapping\nbase_model = BlipForQuestionAnswering.from_pretrained(\n    \"Salesforce/blip-vqa-base\",\n    device_map=\"auto\",\n    load_in_8bit=True,\n    torch_dtype=torch.float16\n)\n\n# Load the processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\nmodel = base_model\n\n# Set the model to evaluation mode\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"99640bdc-06f4-4f82-ad49-a7ffaf828b0a","cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/BARTScore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a33f765-c4ac-4c84-94d0-6c0b314790b9","cell_type":"code","source":"import re\nimport numpy as np\nfrom bart_score import BARTScorer\nfrom sentence_transformers import SentenceTransformer, util\nimport evaluate\n\n# Load metrics\nrouge = evaluate.load(\"rouge\")\nbertscore = evaluate.load(\"bertscore\")\nembedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\nbart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\n\n# Post-processing utilities\ndef clean_answer(answer):\n    \"\"\"Return only the first word (alphanumeric).\"\"\"\n    return re.findall(r\"\\b\\w+\\b\", answer.strip().lower())[0] if re.findall(r\"\\b\\w+\\b\", answer.strip().lower()) else \"\"\n\n# Metric functions\ndef compute_exact_match(pred, label):\n    return int(pred == label)\n\ndef compute_token_f1(pred, label):\n    pred_tokens = pred.split()\n    label_tokens = label.split()\n    common = set(pred_tokens) & set(label_tokens)\n    if not common:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(label_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef compute_semantic_similarity(pred, label):\n    emb_pred = embedding_model.encode(pred, convert_to_tensor=True)\n    emb_label = embedding_model.encode(label, convert_to_tensor=True)\n    return float(util.cos_sim(emb_pred, emb_label))\n\n# Prediction function\ndef predict_blip(image_path, question):\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=20,\n            do_sample=False,\n            num_beams=1\n        )\n    decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n    return clean_answer(decoded)\n\n# Main evaluation\ndef evaluate_blip(dataset, image_folder):\n    exact_matches = []\n    token_f1s = []\n    rouge_scores = []\n    bert_scores = []\n    bart_scores = []\n    semantic_similarities = []\n\n    for sample in tqdm(dataset, desc=\"Evaluating BLIP\"):\n        image_name = sample[\"image_name\"]\n        question = sample[\"question\"]\n        reference = clean_answer(sample[\"answer\"])\n\n        image_path = os.path.join(image_folder, image_name)\n        prediction = predict_blip(image_path, question)\n\n        # Compute metrics\n        exact_matches.append(compute_exact_match(prediction, reference))\n        token_f1s.append(compute_token_f1(prediction, reference))\n        rouge_result = rouge.compute(predictions=[prediction], references=[reference], use_stemmer=True)\n        rouge_scores.append(rouge_result[\"rougeL\"])\n        bert_result = bertscore.compute(predictions=[prediction], references=[reference], lang=\"en\")\n        bert_scores.append(bert_result[\"f1\"][0])\n        bart = bart_scorer.score([prediction], [reference])[0]\n        bart_scores.append(bart)\n        semantic_similarities.append(compute_semantic_similarity(prediction, reference))\n\n    # Final results\n    print(\"\\nüîç Evaluation Metrics for BLIP:\")\n    print(f\"  - Exact Match:            {np.mean(exact_matches):.4f}\")\n    print(f\"  - Token-level F1:         {np.mean(token_f1s):.4f}\")\n    print(f\"  - ROUGE-L:                {np.mean(rouge_scores):.4f}\")\n    print(f\"  - BERTScore (F1):         {np.mean(bert_scores):.4f}\")\n    print(f\"  - BARTScore:              {np.mean(bart_scores):.4f}\")\n    print(f\"  - Semantic Cosine Sim.:   {np.mean(semantic_similarities):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"02e5352f-ef4c-41f6-9bf6-f8592b830cea","cell_type":"code","source":"evaluate_blip(test_dataset, image_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}