{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11763225,"sourceType":"datasetVersion","datasetId":7384866}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"# Install necessary packages\n!pip install -U transformers accelerate peft bitsandbytes\n!pip install backoff\n!pip install flash-attn --no-build-isolation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Imports\nimport os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoProcessor, AutoModelForCausalLM,\n    TrainingArguments, Trainer\n)\nfrom peft import LoraConfig, get_peft_model\nfrom torch.utils.data import Dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the model","metadata":{}},{"cell_type":"code","source":"# Paths\nimage_folder = \"/kaggle/input/vr-dataset-final/images/images\"\ncsv_path = \"/kaggle/input/vr-dataset-final/annotations.csv\"\n\nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\"\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\nmodel     = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    attn_implementation=\"eager\"\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=\"all-linear\"\n\n)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"class Phi3VQADataset(Dataset):\n    def __init__(self, csv_path, image_folder, processor, max_samples=None):\n        self.data = pd.read_csv(csv_path)\n        if max_samples:\n            self.data = self.data[:max_samples]\n        self.image_folder = image_folder\n        self.processor = processor\n        self.image_token = processor.img_tokens[0]  # \"<|image_1|>\"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Create the prompt for the question\n        prompt = f\"[INST] {row['question']} {self.image_token} [/INST]\"\n        answer = row[\"answer\"]\n\n        # Process the text and image\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=256\n        )\n\n        # Tokenize the one-word answer directly\n        labels = self.processor.tokenizer(\n            answer,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=30\n        ).input_ids\n\n        # Squeeze the batch dimension\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs[\"labels\"] = labels.squeeze(0)\n\n        return inputs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataCollator:\n    def __init__(self, processor, image_dir):\n        self.processor = processor\n        self.image_dir = image_dir\n\n    def __call__(self, examples):\n        example = examples[0]\n        print(example)\n        # Load image using image_name\n        image_path = os.path.join(self.image_dir, example[\"image_name\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        user_prompt = f\"Answer the question about this image: {example['question']}\"\n        answer = example[\"answer\"]\n\n        # Prepare chat-style messages\n        messages = [\n            {\"role\": \"user\", \"content\": f\"<|image_1|>\\n{user_prompt}\"}\n        ]\n\n        # Tokenize prompt with chat template\n        prompt = self.processor.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        answer = f\"{answer}<|end|>\\n<|endoftext|>\"\n\n        # Process image and prompt\n        batch = self.processor(prompt, [image], return_tensors=\"pt\")\n        prompt_input_ids = batch[\"input_ids\"]\n\n        # Tokenize the one-word answer\n        answer_input_ids = self.processor.tokenizer(\n            answer, add_special_tokens=False, return_tensors=\"pt\"\n        )[\"input_ids\"]\n\n        # Combine prompt and answer tokens\n        concatenated_input_ids = torch.cat([prompt_input_ids, answer_input_ids], dim=1)\n\n        # Create labels: ignore prompt tokens\n        ignore_index = -100\n        labels = torch.cat(\n            [\n                torch.full_like(prompt_input_ids, ignore_index),\n                answer_input_ids,\n            ],\n            dim=1,\n        )\n\n        # Final batch dictionary\n        batch[\"input_ids\"] = concatenated_input_ids\n        batch[\"labels\"] = labels\n\n        # Ensure gradients only for float tensors\n        for key, value in batch.items():\n            if isinstance(value, torch.Tensor) and torch.is_floating_point(value):\n                batch[key] = value.clone().detach().requires_grad_(True)\n\n        return batch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Set up the dataset\nfull_dataset = Phi3VQADataset(csv_path, image_folder, processor, max_samples=5)\ntrain_size = int(0.8 * len(full_dataset))\nval_size = int(0.1 * len(full_dataset))\ntest_size = len(full_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    full_dataset, [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./phi4-mm-vqa\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=True,\n    report_to=\"none\",\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\", \n    logging_steps=5,\n    eval_strategy=\"steps\",      \n    eval_steps=5,                   \n    save_strategy=\"steps\",            \n    save_steps=5,                   \n    save_total_limit=2,              \n    load_best_model_at_end=True,      \n    metric_for_best_model=\"eval_loss\", \n    greater_is_better=False,         \n    label_names=[\"labels\"]\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize trainer with custom data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=processor.tokenizer,\n    # data_collator=DataCollator(processor,image_folder)\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def phi4_infer(image_path, question):\n    image = Image.open(image_path).convert(\"RGB\")\n    prompt = f\"[INST] {question} [/INST]\"\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=10)\n    \n    return processor.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n\n# Test example\nrow = pd.read_csv(csv_path).iloc[123]\nimg_path = os.path.join(image_folder, row[\"image_name\"])\nprint(\"Q:\", row[\"question\"])\nprint(\"A (pred):\", phi4_infer(img_path, row[\"question\"]))\nprint(\"A (GT):\", row[\"answer\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}