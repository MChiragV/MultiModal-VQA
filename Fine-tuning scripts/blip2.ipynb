{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11801135,"sourceType":"datasetVersion","datasetId":7407267}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers accelerate peft\n!pip install bitsandbytes\n!pip install -U evaluate\n!pip install -U rouge_score","metadata":{"_uuid":"0e4e96d1-6559-420d-bee5-32d349a4fbbe","_cell_guid":"1684ae36-8576-4ca2-a966-25fe293d58e6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports\n","metadata":{}},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nfrom peft import LoraConfig, get_peft_model,PeftModel\nfrom transformers import TrainingArguments, Trainer\nimport os\nfrom torch.utils.data import Dataset\nimport evaluate\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PATHS","metadata":{}},{"cell_type":"code","source":"# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nimage_folder = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"  # <-- Change this\ncsv_path = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"  # <-- Change this","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Model","metadata":{}},{"cell_type":"code","source":"processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-flan-t5-xl\",\n    device_map={\"\": 0},\n    load_in_8bit=True,\n    torch_dtype=torch.float16\n)\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=\"all-linear\"\n\n)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, csv_path, image_folder, processor, max_samples=None):\n        self.data = pd.read_csv(csv_path)\n\n        if max_samples is not None:\n            self.data = self.data[:max_samples]  # Take only the first max_samples rows\n\n        self.image_folder = image_folder\n        self.processor = processor\n\n        print(f\"[INFO] Loaded {len(self.data)} samples from '{csv_path}'\")\n        print(f\"[INFO] Image folder: {image_folder}\")\n\n    def __len__(self):\n        return len(self.data)\n        \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        image_path = os.path.join(self.image_folder, row['image_name'])\n        image = Image.open(image_path).convert(\"RGB\")\n    \n        question = row['question']\n        answer = row['answer']\n    \n        # Prepare inputs (question + image)\n        inputs = self.processor(images=image, text=question, return_tensors=\"pt\",\n                                padding=\"max_length\", truncation=True, max_length=128)\n        inputs = {k: v.squeeze(0).to(device) for k, v in inputs.items()}\n    \n        # Get the first word (before space), strip to be safe\n        first_word = answer.strip().split()[0]\n    \n        # Tokenize just the first word (may result in multiple tokens)\n        tokenized = self.processor.tokenizer(first_word, return_tensors=\"pt\",\n                                             padding=\"max_length\", truncation=True, max_length=128)\n    \n        input_ids = tokenized[\"input_ids\"].squeeze(0)\n        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n    \n        # Create labels: only keep first-word tokens (those with attention mask = 1), rest are -100\n        labels = torch.where(attention_mask == 1, input_ids, torch.full_like(input_ids, -100)).to(device)\n\n    \n        inputs['labels'] = labels\n    \n        return inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Load dataset\nfull_dataset = VQADataset(csv_path, image_folder, processor,5)\n\n# # Split the dataset\ntrain_size = int(0.8 * len(full_dataset))\ntrain_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset) - train_size])\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./blip2-vqa-lora\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    num_train_epochs=40,\n    learning_rate=2e-4,\n    save_total_limit=2,\n    weight_decay=0.01,\n    fp16=True,\n    report_to=\"none\",\n    logging_dir=\"./logs\",                       # optional\n    logging_strategy=\"epoch\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = train_dataset[0]\nfor k, v in sample.items():\n    print(f\"{k}: shape = {v.shape}, dtype = {v.dtype}\")\n    if k in ['input_ids']:\n        print(f\"{k} tokens: {processor.tokenizer.decode(v.tolist(), skip_special_tokens=True)}\")\n    elif k == 'labels':\n        valid_token_ids = v[v != -100].tolist()  # Remove -100 before decoding\n        print(f\"{k} tokens: {processor.tokenizer.decode(valid_token_ids, skip_special_tokens=True)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=processor.tokenizer\n)\n\n# Train\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"bleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\naccuracy_metric = evaluate.load(\"accuracy\")\n\n\ncheckpoint_path = \"/kaggle/working/blip2-vqa-lora/checkpoint-500\"\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-flan-t5-xl\",\n    device_map={\"\": 0},\n    load_in_8bit=True,\n    torch_dtype=torch.float16\n)\n\nmodel = PeftModel.from_pretrained(base_model, checkpoint_path).eval()\n\n\n\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch in tqdm(test_loader):\n    pixel_values = batch['pixel_values'].to(\"cuda:1\")  # shape: [1, 3, H, W]\n    input_ids = batch['input_ids'].to(\"cuda:1\")      # question tokens\n    labels = batch['labels'].to(\"cuda:1\")              # ground truth answer tokens\n\n    with torch.no_grad():\n        outputs = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_new_tokens=30)\n        pred = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n        ref = processor.tokenizer.decode(labels[0], skip_special_tokens=True)\n\n    predictions.append(pred.strip())\n    references.append(ref.strip())\n\n# BLEU expects list of list of references and list of predictions\nbleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\nrouge_score = rouge.compute(predictions=predictions, references=references)\n\n# Accuracy (exact match)\nexact_matches = [int(p.lower().strip() == r.lower().strip()) for p, r in zip(predictions, references)]\naccuracy = sum(exact_matches) / len(exact_matches)\n\nprint(\"BLEU:\", bleu_score)\nprint(\"ROUGE:\", rouge_score)\nprint(\"Exact Match Accuracy:\", accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(image_path, question):\n    # Open image\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # Prepare inputs\n    inputs = processor(text=question, images=image, return_tensors=\"pt\").to(model.device)\n\n    # Generate the answer using the model\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=5,  # You can change max_new_tokens as needed\n            do_sample=False,  # You can set this to True if you want randomness\n            num_beams=1\n        )\n\n    # Decode the output to get the predicted answer\n    decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return decoded.strip(), image\n\ndf = pd.read_csv(csv_path)\n\n# Example to display the result\nrow = df.iloc[20]  # Adjust this to select a row from your dataframe\nimage_name = row[\"image_name\"]  # Assuming 'image_name' is the column in your CSV\nquestion = row[\"question\"]\n\n# Construct the full image path\nimage_path = os.path.join(image_folder, image_name)  # This joins the directory and the image name\n\n# Predict the answer using the model\npredicted_answer, image = predict(image_path, question)\n\n# Display the image along with the question and predicted answer\nplt.figure(figsize=(5, 5))\nplt.imshow(image)\nplt.axis('off')\nplt.title(f\"Q: {question}\\nPredicted A: {predicted_answer}\\nGT: {row['answer']}\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}