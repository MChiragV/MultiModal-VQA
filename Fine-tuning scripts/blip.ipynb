{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11801135,"sourceType":"datasetVersion","datasetId":7407267}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ac260700-6a21-42b0-9941-b9a08f6b7ed7","cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"id":"87d33c91-07d9-45a0-a41a-9ad488fad407","cell_type":"code","source":"!pip install -U transformers accelerate peft\n!pip install -U bitsandbytes\n!pip install -U evaluate\n!pip install -U rouge_score\n!pip install bert-score\n!git clone https://github.com/neulab/BARTScore.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a8b11e3b-2c5d-45c9-ad14-5f68d2188bdb","cell_type":"code","source":"import wandb\nwandb.login(key=\"YOUR_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b935b387-79e7-4724-a7e8-70e0547e82f9","cell_type":"code","source":"import os\n\nos.environ[\"WANDB_PROJECT\"] = \"blip-lora-final-finetune\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"false\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"53c22132-adea-4478-9233-0827ea67d46f","cell_type":"markdown","source":"# Imports","metadata":{}},{"id":"a4be4456","cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nimport torch\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nfrom peft import LoraConfig, get_peft_model,PeftModel\nfrom transformers import TrainingArguments, Trainer\nimport os\nfrom torch.utils.data import Dataset,DataLoader \nfrom evaluate import load\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"19c722c5","cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nimage_folder = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"  \ncsv_path = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fff10770-8d98-4611-9f2b-85448234c6f9","cell_type":"markdown","source":"## Loading the models","metadata":{}},{"id":"9df9d618","cell_type":"code","source":"processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5b87f688","cell_type":"code","source":"model = BlipForQuestionAnswering.from_pretrained(\n    \"Salesforce/blip-vqa-base\",\n    device_map=\"auto\",\n    load_in_8bit=True,\n    torch_dtype=torch.float16\n)\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=\"all-linear\",\n    task_type = \"QUESTION_ANS\"\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2ae38aab-4b97-462e-8152-b82eb63e8d7e","cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"id":"067092ae-dbb2-4ba8-b675-f319ffb9985e","cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, csv_path, image_folder, processor, max_samples=None):\n        self.data = pd.read_csv(csv_path)\n\n        if max_samples is not None:\n            self.data = self.data[:max_samples]  # Take only the first max_samples rows\n\n        self.image_folder = image_folder\n        self.processor = processor\n\n        print(f\"[INFO] Loaded {len(self.data)} samples from '{csv_path}'\")\n        print(f\"[INFO] Image folder: {image_folder}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        image_path = os.path.join(self.image_folder, row['image_name'])\n        image = Image.open(image_path).convert(\"RGB\")\n    \n        question = row['question']\n        answer = row['answer']\n    \n        # Prepare inputs (question + image)\n        inputs = self.processor(images=image, text=question, return_tensors=\"pt\", \n                                padding=\"max_length\", truncation=True, max_length=128)\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n    \n        # Tokenize answer with padding and truncation\n        tokenized = self.processor.tokenizer(\n            answer,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=10\n        )\n    \n        input_ids = tokenized[\"input_ids\"].squeeze(0)\n        labels = input_ids\n        inputs[\"labels\"] = labels\n\n        # Add metadata for later reference\n        inputs[\"image_name\"] = row[\"image_name\"]\n        inputs[\"question\"] = question\n        inputs[\"answer\"] = answer\n    \n        return inputs\n\n\n\n# Load dataset\nfull_dataset = VQADataset(csv_path, image_folder, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"644dbd13-f68d-47b0-a0e1-c4adc0a55161","cell_type":"code","source":"train_ratio = 0.8\nval_ratio = 0.075\ntest_ratio = 0.125\n\n# Total size of the dataset\ntotal_size = len(full_dataset)\n\n# Compute sizes\ntrain_size = int(train_ratio * total_size)\nval_size = int(val_ratio * total_size)\ntest_size = total_size - train_size - val_size  # Ensures total sums to len(full_dataset)\n\n# Perform the split\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n    full_dataset, [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)  # for reproducibility\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3bce22bd-1b32-4171-abca-b6567d09287a","cell_type":"code","source":"print(test_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b06d9ce8-0f19-474c-a1ce-87085bc5879f","cell_type":"markdown","source":"# Training","metadata":{}},{"id":"b0ca4fc6","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./blip-vqa-lora-final-2\",\n    report_to=\"wandb\",                      # Log to Weights & Biases\n    run_name=\"blip-vqa-final-finetune\",        #Custom run name\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    fp16=True,\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",         # Log every 'logging_steps'\n    logging_steps=1000,                # Log every 500 steps\n    eval_strategy=\"steps\",      # Evaluate every 'eval_steps'\n    eval_steps=1000,                   # Evaluate every 500 steps\n    save_strategy=\"steps\",            # Save checkpoint every 'save_steps'\n    save_steps=1000,                   # Save every 500 steps\n    save_total_limit=2,               # Retain only the 2 most recent checkpoints\n    load_best_model_at_end=True,      # Load the best model at the end of training\n    metric_for_best_model=\"eval_loss\",# Use evaluation loss to determine the best model\n    greater_is_better=False,          # Lower eval_loss indicates a better model\n    label_names=[\"labels\"]            # Specify the correct label name\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6562fb01-0ec7-4a2e-81f0-3055a88364b2","cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  \n    tokenizer=processor.tokenizer\n)\n\n\n# Train\ntrainer.train() # if resume_checkpoint has to be used comment it","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c7061719-0d54-479a-84b2-3b3bbe8af17f","cell_type":"code","source":"trainer.train(resume_from_checkpoint=\"/kaggle/working/blip-vqa-lora-final-1/checkpoint-8000\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"de96fdf5-fa66-4b0a-80d5-d160f6c87b30","cell_type":"code","source":"best_model_path = trainer.state.best_model_checkpoint\nprint(f\"Best model saved at: {best_model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"988558b1-ecfa-4f4e-b9b7-62e9acc19941","cell_type":"markdown","source":"# Pushing to Hugging Face","metadata":{}},{"id":"35e5a61b-5cac-407a-b45b-16234421b89a","cell_type":"code","source":"from huggingface_hub import login, HfApi\n\nlogin(token=\"YOUR_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a9ebd3d6-2176-43eb-ad8e-4cc361380e33","cell_type":"code","source":"from huggingface_hub import HfApi\n\nusername = \"sohith18\"  \nrepo_name = \"blip-lora-vqa\"\nrepo_id = f\"{username}/{repo_name}\"\n\napi = HfApi()\napi.create_repo(repo_id=repo_id, repo_type=\"model\", private=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"107d14cf-84f3-4105-a527-37ec186081b3","cell_type":"code","source":"from huggingface_hub import upload_folder\n\nusername = \"sohith18\" \nrepo_name = \"blip-lora-vqa\"\nrepo_id = f\"{username}/{repo_name}\"\n\nupload_folder(\n    repo_id=repo_id,  \n    folder_path=\"/kaggle/working/blip-vqa-lora-final-1/checkpoint-8000\",  #output_dir\n    path_in_repo=\"\",  # Upload root contents to the repo\n    repo_type=\"model\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0d307a5d-f18f-4c9c-b205-f3b425d8d623","cell_type":"markdown","source":"# Inference","metadata":{}},{"id":"593bfe2d-ce91-4ba1-8fd4-e3f04fa91e30","cell_type":"code","source":"from transformers import BlipForQuestionAnswering, BlipProcessor\nfrom peft import PeftModel\nimport torch\n\nlora_path = \"sohith18/blip-lora-vqa\"\n\n#Load BLIP base model in 8-bit with fp16 and auto device mapping\nbase_model = BlipForQuestionAnswering.from_pretrained(\n    \"Salesforce/blip-vqa-base\",\n    device_map=\"auto\",\n    load_in_8bit=True,\n    torch_dtype=torch.float16\n)\n\n# Load the processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n#Load LoRA weights into the base model\nmodel = PeftModel.from_pretrained(base_model, lora_path)\n\n# Set the model to evaluation mode\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"99640bdc-06f4-4f82-ad49-a7ffaf828b0a","cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/BARTScore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a33f765-c4ac-4c84-94d0-6c0b314790b9","cell_type":"code","source":"import re\nimport numpy as np\nfrom bart_score import BARTScorer\nfrom sentence_transformers import SentenceTransformer, util\nimport evaluate\n\n# Load metrics\nrouge = evaluate.load(\"rouge\")\nbertscore = evaluate.load(\"bertscore\")\nembedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\nbart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\n\n# Post-processing utilities\ndef clean_answer(answer):\n    \"\"\"Return only the first word (alphanumeric).\"\"\"\n    return re.findall(r\"\\b\\w+\\b\", answer.strip().lower())[0] if re.findall(r\"\\b\\w+\\b\", answer.strip().lower()) else \"\"\n\n# Metric functions\ndef compute_exact_match(pred, label):\n    return int(pred == label)\n\ndef compute_token_f1(pred, label):\n    pred_tokens = pred.split()\n    label_tokens = label.split()\n    common = set(pred_tokens) & set(label_tokens)\n    if not common:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(label_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef compute_semantic_similarity(pred, label):\n    emb_pred = embedding_model.encode(pred, convert_to_tensor=True)\n    emb_label = embedding_model.encode(label, convert_to_tensor=True)\n    return float(util.cos_sim(emb_pred, emb_label))\n\n# Prediction function\ndef predict_blip(image_path, question):\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=20,\n            do_sample=False,\n            num_beams=1\n        )\n    decoded = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n    return clean_answer(decoded)\n\n# Main evaluation\ndef evaluate_blip(dataset, image_folder):\n    exact_matches = []\n    token_f1s = []\n    rouge_scores = []\n    bert_scores = []\n    bart_scores = []\n    semantic_similarities = []\n\n    for sample in tqdm(dataset, desc=\"Evaluating BLIP\"):\n        image_name = sample[\"image_name\"]\n        question = sample[\"question\"]\n        reference = clean_answer(sample[\"answer\"])\n\n        image_path = os.path.join(image_folder, image_name)\n        prediction = predict_blip(image_path, question)\n\n        # Compute metrics\n        exact_matches.append(compute_exact_match(prediction, reference))\n        token_f1s.append(compute_token_f1(prediction, reference))\n        rouge_result = rouge.compute(predictions=[prediction], references=[reference], use_stemmer=True)\n        rouge_scores.append(rouge_result[\"rougeL\"])\n        bert_result = bertscore.compute(predictions=[prediction], references=[reference], lang=\"en\")\n        bert_scores.append(bert_result[\"f1\"][0])\n        bart = bart_scorer.score([prediction], [reference])[0]\n        bart_scores.append(bart)\n        semantic_similarities.append(compute_semantic_similarity(prediction, reference))\n\n    # Final results\n    print(\"\\nüîç Evaluation Metrics for BLIP:\")\n    print(f\"  - Exact Match:            {np.mean(exact_matches):.4f}\")\n    print(f\"  - Token-level F1:         {np.mean(token_f1s):.4f}\")\n    print(f\"  - ROUGE-L:                {np.mean(rouge_scores):.4f}\")\n    print(f\"  - BERTScore (F1):         {np.mean(bert_scores):.4f}\")\n    print(f\"  - BARTScore:              {np.mean(bart_scores):.4f}\")\n    print(f\"  - Semantic Cosine Sim.:   {np.mean(semantic_similarities):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"02e5352f-ef4c-41f6-9bf6-f8592b830cea","cell_type":"code","source":"evaluate_blip(test_dataset, image_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}