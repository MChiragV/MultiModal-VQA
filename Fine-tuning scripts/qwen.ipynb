{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers peft bitsandbytes datasets evaluate\n",
    "!pip install qwen-vl-utils[decord]==0.0.8\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install bert-score\n",
    "!git clone https://github.com/neulab/BARTScore.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/BARTScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"YOUR_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"qwen-7B-lora-final-finetune\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Import libraries\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths and model setup\n",
    "CSV_PATH     = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"\n",
    "IMAGE_FOLDER = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"\n",
    "MODEL_ID     = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load processor & tokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "print(\"â†’ image placeholder is:\", tokenizer.additional_special_tokens[0])\n",
    "\n",
    "# Load base model\n",
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules='all-linear',\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QwenDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_folder):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_folder = image_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        instruction = (\n",
    "            \"You must answer with exactly one word. \"\n",
    "            \"Spaces between words will be treated as multiple words. \"\n",
    "            \"Do not include any explanations or punctuation.\"\n",
    "        )\n",
    "        question = row[\"question\"]\n",
    "        full_prompt = f\"{instruction}\\n{question}\"\n",
    "        answer = row[\"answer\"]\n",
    "\n",
    "        text = f\"<|im_start|>user\\n<image>\\n{full_prompt}<|im_end|>\\n<|im_start|>assistant\\n{answer}<|im_end|>\"\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text\": text,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "ds = QwenDataset(CSV_PATH, IMAGE_FOLDER)\n",
    "total_len = len(ds)\n",
    "\n",
    "# Compute sizes\n",
    "train_size = int(0.8 * total_len)\n",
    "val_size = int(0.1 * total_len)\n",
    "test_size = total_len - train_size - val_size  # Remaining for test\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "# Perform split with fixed random seed\n",
    "train_ds, val_ds, test_ds = random_split(ds, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "# Print sizes\n",
    "print(train_size)\n",
    "print(val_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "\n",
    "    messages = []\n",
    "    for text, image in zip(texts, images):\n",
    "        # Parse full conversation: user + assistant\n",
    "        parts = text.strip().split(\"<|im_start|>assistant\")\n",
    "        user_part = parts[0].strip()\n",
    "        assistant_part = parts[1].split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "        messages.append([\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": user_part.split(\"<|im_end|>\")[0].split(\"\\n\")[-1].strip()},\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_part\n",
    "            }\n",
    "        ])\n",
    "\n",
    "    processed_texts = [\n",
    "        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "        for msg in messages\n",
    "    ]\n",
    "    image_inputs, video_inputs = process_vision_info([m[0] for m in messages])\n",
    "\n",
    "    inputs = processor(\n",
    "        text=processed_texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Mask only the assistant's tokens\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    for i, text in enumerate(processed_texts):\n",
    "        assistant_start = text.find(\"<|im_start|>assistant\")\n",
    "        if assistant_start == -1:\n",
    "            continue\n",
    "        # Get index after the newline after <|im_start|>assistant\n",
    "        assistant_content_start = text.find(\"\\n\", assistant_start) + 1\n",
    "        assistant_content_end = text.find(\"<|im_end|>\", assistant_content_start)\n",
    "        assistant_response = text[assistant_content_start:assistant_content_end]\n",
    "    \n",
    "        prefix = text[:assistant_content_start]\n",
    "        prefix_ids = tokenizer(prefix, add_special_tokens=False).input_ids\n",
    "        response_ids = tokenizer(assistant_response, add_special_tokens=False).input_ids\n",
    "    \n",
    "        # Set -100 for everything before assistant's answer\n",
    "        labels[i, :len(prefix_ids)] = -100\n",
    "        # Set -100 for everything after the answer (including <|im_end|>)\n",
    "        labels[i, len(prefix_ids) + len(response_ids):] = -100\n",
    "\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#TrainingArguments & Trainer\n",
    "log_dir = \"/kaggle/working/logs\"  # You can customize this\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/qwen2vl-lora-kaggle-7b-final\",  # Qwen-specific output dir\n",
    "    report_to=\"wandb\",                      # Log to Weights & Biases\n",
    "    run_name=\"qwen-7B-lora-final-finetune\",        # Custom run name\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=log_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1500,\n",
    "    eval_strategy=\"steps\",            \n",
    "    eval_steps=1500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1500,\n",
    "    save_total_limit=2,\n",
    "    max_steps=18000,                          # Limit steps if needed\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,                              # Faster training with fp16\n",
    "    bf16=False,                             # Set True if using bf16-compatible GPU\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"]                 # Required for custom loss masking\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push To HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "login(token=\"YOUR_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train() # comment it if you want to use resume checkpoint\n",
    "trainer.train(resume_from_checkpoint=\"/kaggle/working/qwen2vl-lora-kaggle-7b-final/checkpoint-12000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = \"sohith18\"  \n",
    "repo_name = \"qwen2vl-lora-vqa-7b\"\n",
    "repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"model\", private=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=repo_id,  # Replace with your actual username\n",
    "    folder_path=\"/kaggle/working/qwen2vl-lora-kaggle-7b-final/checkpoint-18000\",  # Your output_dir\n",
    "    path_in_repo=\"\",  # Upload root contents to the repo\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Paths\n",
    "\n",
    "LORA_WEIGHTS = \"sohith18/qwen2vl-lora-vqa-7b\"  # Updated to match your output dir\n",
    "MODEL_ID     = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "#Load base model & LoRA weights\n",
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_WEIGHTS)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "#Load processor and tokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_question_and_answer(text):\n",
    "    user_match = re.search(r\"<\\|im_start\\|>user\\n(?:<image>\\n)?(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n",
    "    raw_user = user_match.group(1).strip() if user_match else \"\"\n",
    "\n",
    "    # Get only the last question sentence (assumes question is last line)\n",
    "    question_lines = [line.strip() for line in raw_user.splitlines() if line.strip()]\n",
    "    question = question_lines[-1] if question_lines else \"\"\n",
    "\n",
    "    assistant_match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n",
    "    answer = assistant_match.group(1).strip() if assistant_match else \"\"\n",
    "\n",
    "    return question, answer\n",
    "\n",
    "def predict(image_path, question,image=None):\n",
    "    if image is None:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # Process the messages correctly\n",
    "    processed_text = processor.apply_chat_template([messages[0]], tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    # Create inputs\n",
    "    inputs = processor(\n",
    "        text=processed_text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Get the length of input for extracting only new tokens\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "     # Generate with proper parameters\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,  # Increased from 20\n",
    "            do_sample=False,\n",
    "            num_beams=1,         # Increased from 1\n",
    "            eos_token_id=tokenizer.eos_token_id,  # Explicitly set EOS token\n",
    "        )\n",
    "\n",
    "    # Extract only the new tokens (the answer)\n",
    "    generated_tokens = output[0][input_len:]\n",
    "\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with one Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load eval data\n",
    "CSV_PATH = \"/kaggle/input/vr-dataset-final-20k/annotations.csv\"\n",
    "IMAGE_FOLDER = \"/kaggle/input/vr-dataset-final-20k/images/unique_images\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"image_path\"] = df[\"image_name\"].apply(lambda x: os.path.join(IMAGE_FOLDER, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "q,ea = extract_question_and_answer(test_ds[i][\"text\"])\n",
    "pred_answer = predict(None,q,test_ds[i][\"image\"])\n",
    "print(f'Question: {q}')\n",
    "first_word = re.split(r'\\W+', pred_answer.strip())[0]\n",
    "print(f'Predicted Answer: {first_word}')\n",
    "print(f'Expected Answer: {ea}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "row = df.iloc[45]\n",
    "ans = predict(row[\"image_path\"], row[\"question\"])\n",
    "\n",
    "# Extract only the first word (split on non-word characters)\n",
    "first_word = re.split(r'\\W+', ans.strip())[0]\n",
    "\n",
    "print(\"Q:\", row[\"question\"])\n",
    "print(\"A:\", first_word)\n",
    "print(\"GT:\", row[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from bart_score import BARTScorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "bart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "# Metric functions\n",
    "def compute_exact_match(pred, label):\n",
    "    return int(pred.strip().lower() == label.strip().lower())\n",
    "\n",
    "def compute_token_f1(pred, label):\n",
    "    pred_tokens = pred.strip().lower().split()\n",
    "    label_tokens = label.strip().lower().split()\n",
    "    common = set(pred_tokens) & set(label_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(label_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "def compute_semantic_similarity(pred, label):\n",
    "    emb_pred = embedding_model.encode(pred, convert_to_tensor=True)\n",
    "    emb_label = embedding_model.encode(label, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(emb_pred, emb_label))\n",
    "\n",
    "\n",
    "def clean_answer(predicted_answer):\n",
    "    # Lowercase and extract alphabetic substrings only\n",
    "    tokens = re.findall(r'[a-zA-Z]+', predicted_answer)\n",
    "\n",
    "    if not tokens:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Remove duplicates while keeping order\n",
    "    seen = set()\n",
    "    tokens = [t for t in tokens if not (t in seen or seen.add(t))]\n",
    "\n",
    "\n",
    "    return tokens[0]\n",
    "\n",
    "\n",
    "def extract_question_and_answer(text):\n",
    "    user_match = re.search(r\"<\\|im_start\\|>user\\n(?:<image>\\n)?(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n",
    "    raw_user = user_match.group(1).strip() if user_match else \"\"\n",
    "\n",
    "    # Get only the last question sentence (assumes question is last line)\n",
    "    question_lines = [line.strip() for line in raw_user.splitlines() if line.strip()]\n",
    "    question = question_lines[-1] if question_lines else \"\"\n",
    "\n",
    "    assistant_match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n",
    "    answer = assistant_match.group(1).strip() if assistant_match else \"\"\n",
    "\n",
    "    return question, answer\n",
    "\n",
    "\n",
    "# Main evaluation\n",
    "def evaluate_qwen(eval_dataset):\n",
    "    exact_matches = []\n",
    "    token_f1s = []\n",
    "    rouge_scores = []\n",
    "    bert_scores = []\n",
    "    bart_scores = []\n",
    "    semantic_similarities = []\n",
    "\n",
    "    for sample in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
    "        image = sample[\"image\"]\n",
    "        text = sample[\"text\"]\n",
    "        question, expected_answer = extract_question_and_answer(text)\n",
    "\n",
    "        # print(question, expected_answer)\n",
    "\n",
    "        # Get prediction\n",
    "        pred_raw = predict(None,question,image)\n",
    "        pred_answer = clean_answer(pred_raw)\n",
    "\n",
    "        print(f'Question: {question}')\n",
    "        print(f'Predicted Answer: {pred_answer}')\n",
    "        print(f'Expected Answer: {expected_answer}')\n",
    "\n",
    "        # Metrics\n",
    "        exact_matches.append(compute_exact_match(pred_answer, expected_answer))\n",
    "        token_f1s.append(compute_token_f1(pred_answer, expected_answer))\n",
    "\n",
    "        rouge_result = rouge.compute(predictions=[pred_answer], references=[expected_answer], use_stemmer=True)\n",
    "        rouge_scores.append(rouge_result[\"rougeL\"])\n",
    "\n",
    "        bert_result = bertscore.compute(predictions=[pred_answer], references=[expected_answer], lang=\"en\")\n",
    "        bert_scores.append(bert_result[\"f1\"][0])\n",
    "\n",
    "        bart_score = bart_scorer.score([pred_answer], [expected_answer])[0]\n",
    "        bart_scores.append(bart_score)\n",
    "\n",
    "        semantic_similarities.append(compute_semantic_similarity(pred_answer, expected_answer))\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n Evaluation Metrics:\")\n",
    "    print(f\"  - Exact Match:            {np.mean(exact_matches):.4f}\")\n",
    "    print(f\"  - Token-level F1:         {np.mean(token_f1s):.4f}\")\n",
    "    print(f\"  - ROUGE-L:                {np.mean(rouge_scores):.4f}\")\n",
    "    print(f\"  - BERTScore (F1):         {np.mean(bert_scores):.4f}\")\n",
    "    print(f\"  - BARTScore:              {np.mean(bart_scores):.4f}\")\n",
    "    print(f\"  - Semantic Cosine Sim.:   {np.mean(semantic_similarities):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "subset_500 = Subset(test_ds, range(500))\n",
    "evaluate_qwen(subset_500)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7407267,
     "sourceId": 11801135,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
